{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom keras.utils.generic_utils import get_custom_objects","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:40:52.420792Z","iopub.execute_input":"2022-04-20T16:40:52.421380Z","iopub.status.idle":"2022-04-20T16:40:58.934224Z","shell.execute_reply.started":"2022-04-20T16:40:52.421275Z","shell.execute_reply":"2022-04-20T16:40:58.933592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [f\"f_{i}\" for i in range(300)]\ntrain = pd.read_pickle(\"../input/ubiquant-market-prediction-half-precision-pickle/train.pkl\")\ninvestment_id = train.pop(\"investment_id\")\ntime_id = train.pop(\"time_id\")\ny = train.pop(\"target\")","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:40:59.305897Z","iopub.execute_input":"2022-04-20T16:40:59.306558Z","iopub.status.idle":"2022-04-20T16:41:12.035296Z","shell.execute_reply.started":"2022-04-20T16:40:59.306509Z","shell.execute_reply":"2022-04-20T16:41:12.034460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.ops import math_ops\ndef correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    \n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-20T16:41:12.037327Z","iopub.execute_input":"2022-04-20T16:41:12.037599Z","iopub.status.idle":"2022-04-20T16:41:12.047490Z","shell.execute_reply.started":"2022-04-20T16:41:12.037568Z","shell.execute_reply":"2022-04-20T16:41:12.046656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dr = 0.1\ndef get_model_cnn_dr01():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(1e-4), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model\n\ndr = 0.3\ndef get_model_cnn_dr03():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(dr)(feature_x)\n\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(1e-4), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model\n\ndr = 0.5\ndef get_model_cnn_dr05():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(dr)(feature_x)\n\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(1e-4), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model\n\ndr = 0.7\ndef get_model_cnn_dr07():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(dr)(feature_x)\n\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(dr)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(1e-4), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-20T16:41:12.048989Z","iopub.execute_input":"2022-04-20T16:41:12.049690Z","iopub.status.idle":"2022-04-20T16:41:12.101672Z","shell.execute_reply.started":"2022-04-20T16:41:12.049647Z","shell.execute_reply":"2022-04-20T16:41:12.100839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_ids = [i for i in range(1,7000)]\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_id\":investment_id}))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:41:12.103014Z","iopub.execute_input":"2022-04-20T16:41:12.103526Z","iopub.status.idle":"2022-04-20T16:41:52.440095Z","shell.execute_reply.started":"2022-04-20T16:41:12.103488Z","shell.execute_reply":"2022-04-20T16:41:52.439299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_ids = [i for i in range(1,5000)]\ntime_id_size = len(time_ids) + 1\ntime_id_lookup_layer = layers.IntegerLookup(max_tokens=time_id_size)\ntime_id_lookup_layer.adapt(pd.DataFrame({\"time_id\":time_id}))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:41:52.441328Z","iopub.execute_input":"2022-04-20T16:41:52.441563Z","iopub.status.idle":"2022-04-20T16:42:32.957418Z","shell.execute_reply.started":"2022-04-20T16:41:52.441534Z","shell.execute_reply":"2022-04-20T16:42:32.956675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hard_swish(x):\n    return x * (K.relu(x + 3., max_value = 6.) / 6.)\n\ndef get_model_ae():\n    investment_id_inputs = keras.Input((1, ), dtype=tf.uint16)\n    time_id_inputs = keras.Input((1,), dtype=tf.uint16)\n    features_inputs = keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1, embeddings_initializer=tf.keras.initializers.GlorotNormal())(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation=hard_swish)(investment_id_x)\n    investment_id_x = tf.keras.layers.Dropout(0.7)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation=hard_swish)(investment_id_x)\n    investment_id_x = tf.keras.layers.Dropout(0.7)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation=hard_swish)(investment_id_x)\n    \n    time_id_x = time_id_lookup_layer(time_id_inputs)\n    time_id_x = layers.Embedding(time_id_size, 8, input_length=1, embeddings_initializer=tf.keras.initializers.GlorotNormal())(time_id_x)\n    time_id_x = layers.Reshape((-1, ))(time_id_x)\n    time_id_x = layers.Dense(16, activation=hard_swish)(time_id_x)\n    time_id_x = tf.keras.layers.Dropout(0.7)(time_id_x)\n    time_id_x = layers.Dense(16, activation=hard_swish)(time_id_x)\n    time_id_x = tf.keras.layers.Dropout(0.7)(time_id_x)\n    time_id_x = layers.Dense(16, activation=hard_swish)(time_id_x)\n    \n    feature_x = layers.Dense(256, activation=hard_swish, kernel_initializer=tf.keras.initializers.GlorotNormal())(features_inputs)\n    encoder = tf.keras.layers.GaussianNoise(0.5)(feature_x)\n    encoder = tf.keras.layers.Dense(256, activation=hard_swish)(feature_x)\n    encoder = tf.keras.layers.experimental.SyncBatchNormalization()(encoder)\n    encoder = tf.keras.layers.Activation(hard_swish)(encoder)\n    decoder = tf.keras.layers.Dropout(0.7)(encoder)\n    decoder = tf.keras.layers.Dense(256, name = 'decoder')(decoder)\n    x_ae = tf.keras.layers.Dense(256)(decoder)\n    x_ae = tf.keras.layers.experimental.SyncBatchNormalization()(x_ae)\n    x_ae = tf.keras.layers.Activation(hard_swish)(x_ae)\n    x_ae = tf.keras.layers.Dropout(0.7)(x_ae)\n    x_ae = tf.keras.layers.Dense(256, activation=tf.keras.layers.LeakyReLU(0.3))(x_ae)    \n    \n    x = layers.Concatenate(axis=1)([investment_id_x, time_id_x, x_ae])\n    x = layers.Dense(512, activation=hard_swish, kernel_regularizer='l2')(x)\n    x = layers.Dense(128, activation=hard_swish, kernel_regularizer='l2')(x)\n    x = layers.Dense(32, activation=hard_swish, kernel_regularizer='l2')(x)\n    output = layers.Dense(1)(x)\n    \n    rmse = keras.metrics.RootMeanSquaredError(name='rmse')\n    model = keras.Model(inputs=[investment_id_inputs, time_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:42:32.958686Z","iopub.execute_input":"2022-04-20T16:42:32.958933Z","iopub.status.idle":"2022-04-20T16:42:32.981240Z","shell.execute_reply.started":"2022-04-20T16:42:32.958904Z","shell.execute_reply":"2022-04-20T16:42:32.980462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final models\nfrom tqdm import tqdm\nmodels_ae_wt_1, models_ae_wt_2, models_ae_wt_3, models_ae_wt_4 = [], [], [], []\nmodels_ae_upd_drgn_1, models_ae_upd_drgn_2 = [], []\nmodels_gru_1, models_gru_2, models_gru_3, models_gru_4 = [], [], [], []\nmodels_cnn_1, models_cnn_2, models_cnn_3, models_cnn_4 = [], [], [], []\n\nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-wt-01/model_{i}\")\n    models_ae_wt_1.append(m)\nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-wt-03/model_{i}\")\n    models_ae_wt_2.append(m)  \nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-wt-05/model_{i}\")\n    models_ae_wt_3.append(m)  \nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-wt-07/model_{i}\")\n    models_ae_wt_4.append(m)\nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-update-drgn-0-05/model_{i}\")\n    models_ae_upd_drgn_1.append(m)  \nfor i in tqdm(range(5)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/ae-update-drgn-05-1/model_{i}\")\n    models_ae_upd_drgn_2.append(m)\nfor i in tqdm(range(3)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/gru-01/model_{i}\")\n    models_gru_1.append(m)\nfor i in tqdm(range(3)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/gru-03/model_{i}\")\n    models_gru_2.append(m)  \nfor i in tqdm(range(3)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/gru-05/model_{i}\")\n    models_gru_3.append(m)  \nfor i in tqdm(range(3)): \n    m = keras.models.load_model(f\"../input/finalmodels/final-models/gru-07/model_{i}\")\n    models_gru_4.append(m)\nfor i in tqdm(range(5)): \n    m = get_model_cnn_dr01()\n    m.load_weights(f\"../input/finalmodels/final-models/cnn-01/model_{i}.tf\")\n    models_cnn_1.append(m)\nfor i in tqdm(range(5)): \n    m = get_model_cnn_dr03()\n    m.load_weights(f\"../input/finalmodels/final-models/cnn-03/model_{i}.tf\")\n    models_cnn_2.append(m)\nfor i in tqdm(range(5)): \n    m = get_model_cnn_dr05()\n    m.load_weights(f\"../input/finalmodels/final-models/cnn-05/model_{i}.tf\")\n    models_cnn_3.append(m)\nfor i in tqdm(range(5)): \n    m = get_model_cnn_dr07()\n    m.load_weights(f\"../input/finalmodels/final-models/cnn-07/model_{i}.tf\")\n    models_cnn_4.append(m)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:42:32.982728Z","iopub.execute_input":"2022-04-20T16:42:32.983033Z","iopub.status.idle":"2022-04-20T16:44:45.848574Z","shell.execute_reply.started":"2022-04-20T16:42:32.983001Z","shell.execute_reply":"2022-04-20T16:44:45.847997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Supplemental models\nmodels_supp_0 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-100-epochs/model_0\")]\nmodels_supp_1 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-500-epochs/model_0\")]\nmodels_supp_2 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-ada03/model_0\")]\nmodels_supp_3 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-henorm-prelu/model_0\")]\nmodels_supp_4 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-sgd-expdec/model_0\")]\nmodels_supp_5 = [keras.models.load_model(f\"../input/suppmodels/supp-models/supp-ae-single-swat/model_0\")]\nm_6 = get_model_ae()\nm_6.load_weights(\"../input/suppmodels/supp-models/supp-ae-single-full-data/model_ae.tf\")\nmodels_supp_6 = [m_6]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:44:45.850493Z","iopub.execute_input":"2022-04-20T16:44:45.850749Z","iopub.status.idle":"2022-04-20T16:44:56.957034Z","shell.execute_reply.started":"2022-04-20T16:44:45.850700Z","shell.execute_reply":"2022-04-20T16:44:56.956116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds, ds_wt=None):\n    y_preds = []\n    for model in models:\n        try:\n            y_pred = model.predict(ds)\n        except ValueError:\n            y_pred = model.predict(ds_wt)\n        y_preds.append(y_pred)\n    res = np.mean(y_preds, axis=0)\n    return res\n\ndef preprocess_test_gru_cnn(feature):\n    return (feature), 0\ndef make_test_dataset_gru_cnn(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.map(preprocess_test_gru_cnn)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference_gru_cnn(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\ndef preprocess_test_wt(investment_id, time_id, feature):\n    return (investment_id, time_id, feature), 0\ndef make_test_dataset_wt(investment_id, time_id, feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, time_id, feature)))\n    ds = ds.map(preprocess_test_wt)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference_wt(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:55:33.578298Z","iopub.execute_input":"2022-04-20T16:55:33.578612Z","iopub.status.idle":"2022-04-20T16:55:33.598073Z","shell.execute_reply.started":"2022-04-20T16:55:33.578574Z","shell.execute_reply":"2022-04-20T16:55:33.597074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nlst_t = 1211\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df[\"time_id\"] = lst_t+1\n    ds_wt = make_test_dataset_wt(test_df[\"investment_id\"], test_df[\"time_id\"], test_df[features])\n    ds_st = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    ds_of = make_test_dataset_gru_cnn(test_df[features])\n    # ae with varying dropouts and gaussian noise\n    t_1 = inference(models_ae_upd_drgn_1, ds_st)\n    t_2 = inference(models_ae_upd_drgn_2, ds_st)\n    # aes with time ids\n    t_3 = inference_wt(models_ae_wt_1, ds_wt)\n    t_4 = inference_wt(models_ae_wt_2, ds_wt)\n    t_5 = inference_wt(models_ae_wt_3, ds_wt)\n    t_6 = inference_wt(models_ae_wt_4, ds_wt)\n    # gru without time id | investment id\n    t_7 = inference_gru_cnn(models_gru_1, ds_of)\n    t_8 = inference_gru_cnn(models_gru_2, ds_of)\n    t_9 = inference_gru_cnn(models_gru_3, ds_of)\n    t_10 = inference_gru_cnn(models_gru_4, ds_of)\n    # cnn without time id or investment id\n    t_11 = inference_gru_cnn(models_cnn_1, ds_of)\n    t_12 = inference_gru_cnn(models_cnn_2, ds_of)\n    t_13 = inference_gru_cnn(models_cnn_3, ds_of)\n    t_14 = inference_gru_cnn(models_cnn_4, ds_of)\n    # supplemental models\n    t_15 = inference(models_supp_0, ds_st, ds_wt)\n    t_16 = inference(models_supp_1, ds_st, ds_wt)\n    t_17 = inference(models_supp_2, ds_st, ds_wt)\n    t_18 = inference(models_supp_3, ds_st, ds_wt)\n    t_19 = inference(models_supp_4, ds_st, ds_wt)\n    t_20 = inference(models_supp_5, ds_st, ds_wt)\n    t_21 = inference(models_supp_6, ds_wt)\n    \n    t_f = np.hstack((np.array([t_1, t_2, t_3, t_4, t_5, t_6, t_7, t_8, t_9, t_10, t_11, t_12, t_13, t_14, t_15,\n                              t_16, t_17, t_18, t_19, t_20, t_21])))\n    \n    sample_prediction_df['target'] = np.mean(t_f, axis=1)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T16:44:56.978842Z","iopub.execute_input":"2022-04-20T16:44:56.979281Z","iopub.status.idle":"2022-04-20T16:45:29.511147Z","shell.execute_reply.started":"2022-04-20T16:44:56.979237Z","shell.execute_reply":"2022-04-20T16:45:29.509781Z"},"trusted":true},"execution_count":null,"outputs":[]}]}